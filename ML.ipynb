{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59453ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.optimize as opt\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,plot_roc_curve,auc,accuracy_score,precision_score,recall_score,f1_score\n",
    "from  sklearn import linear_model\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d22c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = 'model.feature.csv'\n",
    "indf = pd.read_csv(infile)\n",
    "print(indf.shape)\n",
    "indf.head()\n",
    "degfile = 'model.deg.csv'\n",
    "degdf = pd.read_csv(degfile)\n",
    "print(degdf.shape)\n",
    "print(degdf)\n",
    "deg = degdf['gene_symbol'].tolist()\n",
    "deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e49c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcpt = ['IGH','IGK','IGL','TRA','TRB']\n",
    "richness = [i+'_richness' for i in rcpt]\n",
    "diversity = [i+'_diversity' for i in rcpt]\n",
    "clonality = [i+'_clonality' for i in rcpt]\n",
    "#################################################################\n",
    "feature_list = diversity + richness + clonality + deg\n",
    "max_comb = 5\n",
    "#################################################################\n",
    "outfile = 'cv_model.immune+deg.max_comb'+str(max_comb)+'.results.csv'\n",
    "print(outfile)\n",
    "#############################################################################\n",
    "#进度条\n",
    "immun_num = 0\n",
    "for x in range(1,max_comb+1):\n",
    "    immun_num += math.factorial(len(feature_list))/math.factorial(x)/math.factorial(len(feature_list)-x)\n",
    "total_num = immun_num\n",
    "print('total combs: {}'.format(total_num))\n",
    "one_percent = total_num // 100\n",
    "percents = []\n",
    "for x in range(1,101):\n",
    "    percents.append(x*one_percent)\n",
    "#######################################################################\n",
    "running = 0\n",
    "with open(outfile, 'w') as out:\n",
    "    header = 'tag,feature_number,auc,accuracy,precision,recall,f1'\n",
    "    out.write(header+'\\n')\n",
    "    for n in range(1,max_comb+1):\n",
    "        combs = list(itertools.combinations(feature_list,n))\n",
    "        for comb in combs:   \n",
    "            running += 1\n",
    "            if running in percents:\n",
    "                print('running: {0:2}%, No.{1:<8}, time:{2}'.format(percents.index(running)+1,running,time.ctime()))\n",
    "            model_feature = list(comb)\n",
    "            tag = ''\n",
    "            for feature in list(comb):\n",
    "                tag = tag + '+' + feature\n",
    "            tag = tag[1:]\n",
    "        #######################################\n",
    "            allauc=[]; accuracy=[]; precision=[]; recall=[]; f1=[]\n",
    "            for cv in ['cv1','cv2','cv3']:\n",
    "                traindf = indf[indf[cv]=='train']\n",
    "                testdf = indf[indf[cv]=='test']\n",
    "                trainx = traindf.loc[:,model_feature]\n",
    "                trainy = traindf.loc[:,'type']\n",
    "                testx = testdf.loc[:,model_feature]\n",
    "                testy = testdf.loc[:,'type']\n",
    "                lr = linear_model.LogisticRegression()\n",
    "                lr.fit(trainx, trainy)\n",
    "                test_prob = lr.predict_proba(testx)\n",
    "                allauc.append(roc_auc_score(testy, test_prob, multi_class='ovr'))\n",
    "                accuracy.append(accuracy_score(testy, lr.predict(testx)))\n",
    "                precision.append(precision_score(testy, lr.predict(testx), average='macro'))\n",
    "                recall.append(recall_score(testy, lr.predict(testx), average='macro'))\n",
    "                f1.append(f1_score(testy, lr.predict(testx), average='macro'))\n",
    "            feature_number = len(model_feature)\n",
    "            line = tag+','+str(feature_number)+','+str(np.mean(allauc))+','+str(np.mean(accuracy))+','+str(np.mean(precision))+','+str(np.mean(recall))+','+str(np.mean(f1))\n",
    "            out.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation best_model\n",
    "n_best = {'immune':20,'deg':7,'immune+deg':469}\n",
    "feature_comb = 'immune+deg'\n",
    "feature_comb_file = 'cv_model.'+feature_comb+'.max_comb5.results.csv'\n",
    "combdf = pd.read_csv(feature_comb_file)\n",
    "tag_list = combdf.loc[:n_best[feature_comb]-1,'tag']\n",
    "print(tag_list)\n",
    "infile = 'model.feature.csv'\n",
    "indf = pd.read_csv(infile)\n",
    "print(indf.shape)\n",
    "# indf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'model.best_validation.'+feature_comb+'.csv'\n",
    "with open(outfile, 'w') as out:\n",
    "    header = 'tag,feature_number,auc,accuracy,precision,recall,f1'\n",
    "    out.write(header+'\\n')\n",
    "    for tag in tag_list:\n",
    "        model_feature = tag.split('+')\n",
    "        cv = 'cv1'\n",
    "        traindf = indf[indf[cv] != 'validation']\n",
    "        testdf = indf[indf[cv] == 'validation']\n",
    "        trainx = traindf.loc[:,model_feature]\n",
    "        trainy = traindf.loc[:,'type']\n",
    "        testx = testdf.loc[:,model_feature]\n",
    "        testy = testdf.loc[:,'type']\n",
    "        lr = linear_model.LogisticRegression()\n",
    "        lr.fit(trainx, trainy)\n",
    "        test_prob = lr.predict_proba(testx)\n",
    "        allauc = roc_auc_score(testy, test_prob, labels=lr.classes_, multi_class='ovr')\n",
    "        accuracy = accuracy_score(testy, lr.predict(testx))\n",
    "        precision = precision_score(testy, lr.predict(testx), average='macro')\n",
    "        recall = recall_score(testy, lr.predict(testx), average='macro')\n",
    "        f1 = f1_score(testy, lr.predict(testx), average='macro')\n",
    "        feature_number = len(model_feature)\n",
    "        line = '{0},{1},{2},{3},{4},{5},{6}'.format(tag,feature_number,allauc,accuracy,precision,recall,f1)\n",
    "        out.write(line+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b4eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning]",
   "language": "python",
   "name": "conda-env-machine_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
